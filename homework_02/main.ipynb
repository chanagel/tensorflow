{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data prep\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "train_ds = tfds.load('mnist', split = 'train')\n",
    "\n",
    "# extracting image and labels\n",
    "train_ds = train_ds.map(lambda feature_dict: (feature_dict['image'], feature_dict['label']))\n",
    "# reshape from, 28,28,1 to one vector\n",
    "train_ds = train_ds.map(lambda image, label: (tf.reshape(image,(-1,)), label))\n",
    "# rescaling the values\n",
    "train_ds = train_ds.map(lambda image, label: ((tf.cast(image, tf.float32)/128)-1, label))\n",
    "# one-hot encoder\n",
    "train_ds = train_ds.map(lambda image, label: (image, tf.one_hot(label, depth=10)))\n",
    "# taking batches of 64 out of 1024\n",
    "train_ds = train_ds.shuffle(1024).batch(64)\n",
    "# always having 4 minibatches ready for the gpu (to minimize the runtime); splitting work between cpu and gpu\n",
    "train_ds = train_ds.prefetch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing test_data\n",
    "test_ds = tfds.load('mnist', split = 'test')\n",
    "\n",
    "# extracting image and labels\n",
    "test_ds = test_ds.map(lambda feature_dict: (feature_dict['image'], feature_dict['label']))\n",
    "# reshape from, 28,28,1 to one vector\n",
    "test_ds = test_ds.map(lambda image, label: (tf.reshape(image,(-1,)), label))\n",
    "# rescaling the values\n",
    "test_ds = test_ds.map(lambda image, label: ((tf.cast(image, tf.float32)/128)-1, label))\n",
    "# one-hot encoder\n",
    "test_ds = test_ds.map(lambda image, label: (image, tf.one_hot(label, depth=10)))\n",
    "# taking batches of 64 out of 1024\n",
    "test_ds = test_ds.shuffle(1024).batch(64)\n",
    "# always having 4 minibatches ready for the gpu (to minimize the runtime); splitting work between cpu and gpu\n",
    "test_ds = test_ds.prefetch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation via Subclassing from tf.keras.Model\n",
    "class MLP_Model(tf.keras.Model):\n",
    "  def __init__(self, layer_sizes, output_size=10):\n",
    "    super().__init__()\n",
    "    self.mlp_layers = []\n",
    "    # layer_sizes e.g. [256, 256]\n",
    "    for layer_size in layer_sizes:\n",
    "      new_layer = tf.keras.layers.Dense(units=layer_size, activation='sigmoid')\n",
    "      self.mlp_layers.append(new_layer)\n",
    "    self.output_layer = tf.keras.layers.Dense(units=output_size, activation='softmax')\n",
    "\n",
    "  def call(self, x):\n",
    "    for layer in self.mlp_layers:\n",
    "      x = layer(x)\n",
    "    y = self.output_layer(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calc(y_true, y_pred):\n",
    "  equal_check = tf.equal(tf.argmax(y_true, axis=1), tf.argmax(y_pred, axis=1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(equal_check, 'float32'))\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(train_losses, train_accuracies, test_losses, test_accuracies):\n",
    "  plt.figure()\n",
    "  line1, = plt.plot(train_losses, \"b-\")\n",
    "  line2, = plt.plot(test_losses, \"r-\")\n",
    "  line3, = plt.plot(train_accuracies, \"b:\")\n",
    "  line4, = plt.plot(test_accuracies, \"r:\")\n",
    "  plt.xlabel(\"Training steps\")\n",
    "  plt.ylabel(\"Loss/Accuracy\")\n",
    "  plt.legend((line1, line2, line3, line4), (\"training loss\", \"test loss\", \"train accuracy\", \"test accuracy\"))\n",
    "  plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, Visualization of basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "EPOCHS = 10\n",
    "\n",
    "model = MLP_Model(layer_sizes=[256, 256])\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.01) # learning_rate\n",
    "#ds = ds\n",
    "\n",
    "total_train_losses = []\n",
    "total_train_accuracy = []\n",
    "total_test_losses = []\n",
    "total_test_accuracy = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_losses = []\n",
    "  train_accuracy = []\n",
    "  test_losses = []\n",
    "  test_accuracy = []\n",
    "\n",
    "  for x, target in train_ds:\n",
    "    with tf.GradientTape() as tape:\n",
    "      pred = model(x)\n",
    "      loss = cce(target, pred)\n",
    "    # GradientTape calculates gradient already, we don't want to calc them again\n",
    "    # that's why we have to switch indentations\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "    train_losses.append(loss.numpy())\n",
    "    train_accuracy_val = accuracy_calc(target, pred)\n",
    "    train_accuracy.append(train_accuracy_val)\n",
    "\n",
    "  for x, target in test_ds:\n",
    "    pred = model(x)\n",
    "    loss = cce(target, pred)\n",
    "    test_losses.append(loss.numpy())\n",
    "    test_accuracy_val = accuracy_calc(target, pred)\n",
    "    test_accuracy.append(test_accuracy_val)\n",
    "\n",
    "  total_train_losses.append(numpy.mean(train_losses))\n",
    "  total_train_accuracy.append(numpy.mean(train_accuracy))\n",
    "  total_test_losses.append(numpy.mean(test_losses))\n",
    "  total_test_accuracy.append(numpy.mean(test_accuracy))\n",
    "\n",
    "  #print(f'train losses: ', numpy.mean(train_losses))\n",
    "  #print(f'train accuracy: ', numpy.mean(train_accuracy))\n",
    "  #print(f'test losses: ', numpy.mean(test_losses))\n",
    "  #print(f'test accuracy: ', numpy.mean(test_accuracy))\n",
    "\n",
    "# visualization\n",
    "visualization(total_train_losses, total_train_accuracy, total_test_losses, total_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying differnt Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying learning_rate (from 0.01 to 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying learning rate\n",
    "EPOCHS = 10\n",
    "\n",
    "model = MLP_Model(layer_sizes=[256, 256])\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.1) \n",
    "\n",
    "total_train_losses = []\n",
    "total_train_accuracy = []\n",
    "total_test_losses = []\n",
    "total_test_accuracy = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_losses = []\n",
    "  train_accuracy = []\n",
    "  test_losses = []\n",
    "  test_accuracy = []\n",
    "\n",
    "  for x, target in train_ds:\n",
    "    with tf.GradientTape() as tape:\n",
    "      pred = model(x)\n",
    "      loss = cce(target, pred)\n",
    "    # GradientTape calculates gradient already, we don't want to calc them again\n",
    "    # that's why we have to switch indentations\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "    train_losses.append(loss.numpy())\n",
    "    train_accuracy_val = accuracy_calc(target, pred)\n",
    "    train_accuracy.append(train_accuracy_val)\n",
    "\n",
    "  for x, target in test_ds:\n",
    "    pred = model(x)\n",
    "    loss = cce(target, pred)\n",
    "    test_losses.append(loss.numpy())\n",
    "    test_accuracy_val = accuracy_calc(target, pred)\n",
    "    test_accuracy.append(test_accuracy_val)\n",
    "\n",
    "  total_train_losses.append(numpy.mean(train_losses))\n",
    "  total_train_accuracy.append(numpy.mean(train_accuracy))\n",
    "  total_test_losses.append(numpy.mean(test_losses))\n",
    "  total_test_accuracy.append(numpy.mean(test_accuracy))\n",
    "\n",
    "# visualization\n",
    "visualization(total_train_losses, total_train_accuracy, total_test_losses, total_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly higher learning rate seems to improve our Model (lower starting loss, higher overall accuracy). Based on prior knowledge, we can say that this is only the case up to a specific value since otherwise previous learned things will not be acknowledged as much as they should. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying amount of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying layers\n",
    "import numpy\n",
    "EPOCHS = 10\n",
    "\n",
    "model = MLP_Model(layer_sizes=[256, 256, 256, 256])\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.01) \n",
    "\n",
    "total_train_losses = []\n",
    "total_train_accuracy = []\n",
    "total_test_losses = []\n",
    "total_test_accuracy = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_losses = []\n",
    "  train_accuracy = []\n",
    "  test_losses = []\n",
    "  test_accuracy = []\n",
    "\n",
    "  for x, target in train_ds:\n",
    "    with tf.GradientTape() as tape:\n",
    "      pred = model(x)\n",
    "      loss = cce(target, pred)\n",
    "    # GradientTape calculates gradient already, we don't want to calc them again\n",
    "    # that's why we have to switch indentations\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "    train_losses.append(loss.numpy())\n",
    "    train_accuracy_val = accuracy_calc(target, pred)\n",
    "    train_accuracy.append(train_accuracy_val)\n",
    "\n",
    "  for x, target in test_ds:\n",
    "    pred = model(x)\n",
    "    loss = cce(target, pred)\n",
    "    test_losses.append(loss.numpy())\n",
    "    test_accuracy_val = accuracy_calc(target, pred)\n",
    "    test_accuracy.append(test_accuracy_val)\n",
    "\n",
    "  total_train_losses.append(numpy.mean(train_losses))\n",
    "  total_train_accuracy.append(numpy.mean(train_accuracy))\n",
    "  total_test_losses.append(numpy.mean(test_losses))\n",
    "  total_test_accuracy.append(numpy.mean(test_accuracy))\n",
    "\n",
    "# visualization\n",
    "visualization(total_train_losses, total_train_accuracy, total_test_losses, total_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the Model with three more layers does not mean that the Model get more accurate. It seems more like that it is the other way around, since the Model is like this not usable anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying Model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying layers\n",
    "EPOCHS = 10\n",
    "\n",
    "model = MLP_Model(layer_sizes=[10, 10])\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.01) \n",
    "\n",
    "total_train_losses = []\n",
    "total_train_accuracy = []\n",
    "total_test_losses = []\n",
    "total_test_accuracy = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_losses = []\n",
    "  train_accuracy = []\n",
    "  test_losses = []\n",
    "  test_accuracy = []\n",
    "\n",
    "  for x, target in train_ds:\n",
    "    with tf.GradientTape() as tape:\n",
    "      pred = model(x)\n",
    "      loss = cce(target, pred)\n",
    "    # GradientTape calculates gradient already, we don't want to calc them again\n",
    "    # that's why we have to switch indentations\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "    train_losses.append(loss.numpy())\n",
    "    train_accuracy_val = accuracy_calc(target, pred)\n",
    "    train_accuracy.append(train_accuracy_val)\n",
    "\n",
    "  for x, target in test_ds:\n",
    "    pred = model(x)\n",
    "    loss = cce(target, pred)\n",
    "    test_losses.append(loss.numpy())\n",
    "    test_accuracy_val = accuracy_calc(target, pred)\n",
    "    test_accuracy.append(test_accuracy_val)\n",
    "\n",
    "  total_train_losses.append(numpy.mean(train_losses))\n",
    "  total_train_accuracy.append(numpy.mean(train_accuracy))\n",
    "  total_test_losses.append(numpy.mean(test_losses))\n",
    "  total_test_accuracy.append(numpy.mean(test_accuracy))\n",
    "\n",
    "# visualization\n",
    "visualization(total_train_losses, total_train_accuracy, total_test_losses, total_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutting down the layer size drastically shows that the Model is still improving with time, but it takes way more epochs than with more Perceptrons in one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: we need to reprepare the whole data for this since the batch size is set in this first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reprepare train_data to change batch size\n",
    "train_ds = tfds.load('mnist', split = 'train')\n",
    "\n",
    "# extracting image and labels\n",
    "train_ds = train_ds.map(lambda feature_dict: (feature_dict['image'], feature_dict['label']))\n",
    "# reshape from, 28,28,1 to one vector\n",
    "train_ds = train_ds.map(lambda image, label: (tf.reshape(image,(-1,)), label))\n",
    "# rescaling the values\n",
    "train_ds = train_ds.map(lambda image, label: ((tf.cast(image, tf.float32)/128)-1, label))\n",
    "# one-hot encoder\n",
    "train_ds = train_ds.map(lambda image, label: (image, tf.one_hot(label, depth=10)))\n",
    "# taking batches of 64 out of 1024\n",
    "train_ds = train_ds.shuffle(1024).batch(10)\n",
    "# always having 4 minibatches ready for the gpu (to minimize the runtime); splitting work between cpu and gpu\n",
    "train_ds = train_ds.prefetch(4)\n",
    "\n",
    "# reprepare test_data\n",
    "test_ds = tfds.load('mnist', split = 'test')\n",
    "\n",
    "# extracting image and labels\n",
    "test_ds = test_ds.map(lambda feature_dict: (feature_dict['image'], feature_dict['label']))\n",
    "# reshape from, 28,28,1 to one vector\n",
    "test_ds = test_ds.map(lambda image, label: (tf.reshape(image,(-1,)), label))\n",
    "# rescaling the values\n",
    "test_ds = test_ds.map(lambda image, label: ((tf.cast(image, tf.float32)/128)-1, label))\n",
    "# one-hot encoder\n",
    "test_ds = test_ds.map(lambda image, label: (image, tf.one_hot(label, depth=10)))\n",
    "# taking batches of 64 out of 1024\n",
    "test_ds = test_ds.shuffle(1024).batch(10)\n",
    "# always having 4 minibatches ready for the gpu (to minimize the runtime); splitting work between cpu and gpu\n",
    "test_ds = test_ds.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "EPOCHS = 10\n",
    "\n",
    "model = MLP_Model(layer_sizes=[256, 256])\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.01)\n",
    "\n",
    "total_train_losses = []\n",
    "total_train_accuracy = []\n",
    "total_test_losses = []\n",
    "total_test_accuracy = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_losses = []\n",
    "  train_accuracy = []\n",
    "  test_losses = []\n",
    "  test_accuracy = []\n",
    "\n",
    "  for x, target in train_ds:\n",
    "    with tf.GradientTape() as tape:\n",
    "      pred = model(x)\n",
    "      loss = cce(target, pred)\n",
    "    # GradientTape calculates gradient already, we don't want to calc them again\n",
    "    # that's why we have to switch indentations\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "    train_losses.append(loss.numpy())\n",
    "    train_accuracy_val = accuracy_calc(target, pred)\n",
    "    train_accuracy.append(train_accuracy_val)\n",
    "\n",
    "  for x, target in test_ds:\n",
    "    pred = model(x)\n",
    "    loss = cce(target, pred)\n",
    "    test_losses.append(loss.numpy())\n",
    "    test_accuracy_val = accuracy_calc(target, pred)\n",
    "    test_accuracy.append(test_accuracy_val)\n",
    "\n",
    "  total_train_losses.append(numpy.mean(train_losses))\n",
    "  total_train_accuracy.append(numpy.mean(train_accuracy))\n",
    "  total_test_losses.append(numpy.mean(test_losses))\n",
    "  total_test_accuracy.append(numpy.mean(test_accuracy))\n",
    "\n",
    "# visualization\n",
    "visualization(total_train_losses, total_train_accuracy, total_test_losses, total_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smaller batch size results in a somewhat higher accuracy and a lower loss. The price we pay to get this better result is that the computing time is bigger than with bigger batches. This may not be too obvious with this dataset, but will be clearer with huge amount of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
